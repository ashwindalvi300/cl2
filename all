#################################### Analyse a given DNA sequence and perform basic sequence #############################################

#Read the DNA sequence from file
with open('dna_sequence-BI.txt', 'r') as file:
    dna_sequence = file.read().strip()

dna_sequence

# 1. Function to calculate GC content
def calculate_gc_content(sequence):
    g_count = sequence.count('G')
    c_count = sequence.count('C')
    gc_content = (g_count + c_count) / len(sequence) * 100
    return gc_content

# 2. Function to find motifs in the DNA sequence
def find_motifs(sequence, motif):
    positions = []
    for i in range(len(sequence) - len(motif) + 1):
        if sequence[i:i + len(motif)] == motif:
            positions.append(i)
    return positions

# 3. Function to identify coding regions (start and stop codons)
def find_coding_regions(sequence):
    start_codon = "ATG"
    stop_codons = ["TAA", "TAG", "TGA"]
    coding_regions = []

    i = 0
    while i < len(sequence) - 2:
        codon = sequence[i:i + 3]
        if codon == start_codon:
            # Find the next stop codon after the start codon
            for j in range(i + 3, len(sequence) - 2, 3):
                stop_codon = sequence[j:j + 3]
                if stop_codon in stop_codons:
                    coding_regions.append((i, j + 3))  # coding region from start to the end of stop codon
                    i = j + 3
                    break
        i += 3

    return coding_regions

# Calculate results
gc_content = calculate_gc_content(dna_sequence)
motif_positions = find_motifs(dna_sequence, "ATG")
coding_regions = find_coding_regions(dna_sequence)



print(f"GC content : {gc_content}%")

print(f"Motif ATG found at positions : {motif_positions}")

if coding_regions:
    print("Coding regions : ")
    for start, end in coding_regions:
        print(f"Start: {start}, End: {end}")
else:
    print("No coding regions found in the sequence.")



################ RNA-Seq Data Analysis. Task: Analyze a provided RNA-Seq dataset and perform differential  gene expression analysis. ###########
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Simulate RNA-Seq Dataset
np.random.seed(42)

genes = [f'gene_{i}' for i in range(1, 101)]
conditions = ['Control', 'Treatment']

samples = [f'sample_{i}' for i in range(1, 11)]
data = np.random.poisson(lam=20, size=(100, 10))

# Simulate differential expression for some genes in Treatment condition
data[0:5, 5:10] += 15


# Create DataFrame
df = pd.DataFrame(data, index=genes, columns=samples)

metadata = pd.DataFrame({'sample': samples,
                          'condition': ['Control']*5 + ['Treatment']*5})


# Step 2: Normalize the Data
df_norm = df.div(df.sum(axis=0), axis=1) * 10**6
df_log = np.log2(df_norm + 1)

def differential_expression(df, metadata):
    results = []
    for gene in df.index:
        y = df_log.loc[gene].values
        X = pd.get_dummies(metadata['condition'], drop_first=True)
        # The line below was modified to cast the DataFrame to float
        X = sm.add_constant(X.astype(float))
        model = sm.OLS(y, X).fit()
        p_value = model.pvalues[1]
        results.append({'gene': gene, 'p_value': p_value})

    results_df = pd.DataFrame(results)
    results_df['adjusted_p_value'] = sm.stats.multipletests(results_df['p_value'], method='fdr_bh')[1]

    return results_df


# Call the differential_expression function to calculate results_df
results_df = differential_expression(df_log, metadata)  # This line was added to call the function


# Filter differentially expressed genes
deg = results_df[results_df['adjusted_p_value'] < 0.05]


# Step 4: Functional Annotation (Simulated Annotations)
annotations = {
    'gene_1': 'Pathway A',
    'gene_2': 'Pathway B',
    'gene_3': 'Pathway C',
    'gene_4': 'Pathway D',
    'gene_5': 'Pathway E',
}
deg['annotation'] = deg['gene'].map(annotations).fillna('Unknown')


# Step 5: Biological Interpretation (Plotting)
plt.figure(figsize=(10, 6))

sns.scatterplot(x='gene', y='adjusted_p_value', hue='annotation', data=deg)
plt.axhline(y=0.05, color='r', linestyle='--')

plt.xlabel('Genes')
plt.ylabel('Adjusted P-Value')

plt.title('Differentially Expressed Genes')
plt.xticks(rotation=90)

plt.legend(title='Annotations')
plt.tight_layout()

plt.show()

# Save results to a CSV file
deg.to_csv('differentially_expressed_genes.csv', index=False)

# Generate the Report
report = f"""
RNA-Seq Data Analysis Report

Differentially Expressed Genes

{deg[['gene', 'adjusted_p_value']]}

Functional Annotations

{deg[['gene', 'annotation']]}

Potential Biological Interpretations

The genes gene_1, gene_2, etc., are involved in pathways A, B, etc.
These pathways are important for understanding the effect of the treatment condition.
"""

# Save the report to a text file
with open('RNASeq_Analysis_Report.txt', 'w') as f:
    f.write(report)

print("Analysis complete. Results saved to 'differentially_expressed_genes.csv' and 'RNASeq_Analysis_Report.txt'.")


################## Protein Structure Prediction. Task: Predict the 3D structure of a given protein sequence ##############

import numpy as np
from Bio import SeqIO, Align
from Bio.PDB import *
import warnings
from Bio import BiopythonWarning

# Suppress Biopython warnings
warnings.simplefilter('ignore', BiopythonWarning)

class ProteinStructurePredictor:
    def __init__(self):
        # Initialize basic amino acid properties
        self.aa_properties = {
            'A': {'hydrophobicity': 1.8, 'volume': 88.6, 'polarity': 0},
            'R': {'hydrophobicity': -4.5, 'volume': 173.4, 'polarity': 1},
            'N': {'hydrophobicity': -3.5, 'volume': 114.1, 'polarity': 1},
            # ... (other amino acids would be defined similarly)
        }
        # Initialize secondary structure propensities
        self.ss_propensities = {
            'A': {'helix': 1.42, 'sheet': 0.83, 'coil': 0.9},
            'R': {'helix': 1.21, 'sheet': 0.93, 'coil': 0.99},
            'N': {'helix': 0.67, 'sheet': 0.89, 'coil': 1.33},
            # ... (other amino acids would be defined similarly)
        }

    def search_template_database(self, query_sequence):
        """
        Simulate searching for template structures in the PDB database
        Returns: mock template data for demonstration
        """
        # Mock template structure
        mock_template = {
            'pdb_id': '1ABC',
            'sequence': 'SIMILAR_SEQUENCE',
            'resolution': 2.5,
            'coordinates': np.random.rand(len(query_sequence), 3)
        }
        return mock_template

    def align_sequences(self, query_sequence, template_sequence):
        """
        Perform sequence alignment between query and template
        """
        aligner = Align.PairwiseAligner()
        aligner.open_gap_score = -10
        aligner.extend_gap_score = -0.5
        alignments = aligner.align(query_sequence, template_sequence)
        return alignments[0]

    def predict_secondary_structure(self, sequence):
        """
        Predict secondary structure using propensities
        """
        structure = []
        window_size = 5
        padded_seq = 'X' * (window_size // 2) + sequence + 'X' * (window_size // 2)
        
        for i in range(len(sequence)):
            window = padded_seq[i:i + window_size]
            helix_score = sheet_score = coil_score = 0

            # Calculate propensity scores
            for aa in window:
                if aa in self.ss_propensities:
                    helix_score += self.ss_propensities[aa]['helix']
                    sheet_score += self.ss_propensities[aa]['sheet']
                    coil_score += self.ss_propensities[aa]['coil']

            # Assign structure based on highest score
            if max(helix_score, sheet_score, coil_score) == helix_score:
                structure.append('H')
            elif max(helix_score, sheet_score, coil_score) == sheet_score:
                structure.append('E')
            else:
                structure.append('C')
        
        return ''.join(structure)

    def build_backbone(self, sequence, secondary_structure):
        """
        Build basic backbone structure using predicted secondary structure
        Returns: numpy array of coordinates
        """
        coordinates = np.zeros((len(sequence), 3))
        current_pos = np.array([0.0, 0.0, 0.0])
        
        for i in range(len(sequence)):
            if secondary_structure[i] == 'H':  # Helix
                current_pos += np.array([1.5, 0.5, 0.5])
            elif secondary_structure[i] == 'E':  # Sheet
                current_pos += np.array([1.5, 0.0, 0.0])
            else:  # Coil
                current_pos += np.array([1.0, np.random.rand(), np.random.rand()])
            coordinates[i] = current_pos
        
        return coordinates

    def refine_structure(self, coordinates):
        """
        Simple structure refinement
        """
        # Apply basic energy minimization (simplified)
        refined_coords = coordinates + np.random.normal(0, 0.1, coordinates.shape)
        return refined_coords

    def predict_structure(self, sequence):
        """
        Main method to predict protein structure
        """
        # Step 1: Search for template
        template = self.search_template_database(sequence)
        
        # Step 2: Align sequences
        alignment = self.align_sequences(sequence, template['sequence'])
        
        # Step 3: Predict secondary structure
        secondary_structure = self.predict_secondary_structure(sequence)
        
        # Step 4: Build initial backbone
        initial_coords = self.build_backbone(sequence, secondary_structure)
        
        # Step 5: Refine structure
        final_coords = self.refine_structure(initial_coords)
        
        return {
            'coordinates': final_coords,
            'secondary_structure': secondary_structure,
            'alignment': alignment,
            'template_used': template['pdb_id']
        }

    def save_structure(self, coordinates, sequence, filename):
        """
        Save predicted structure in PDB format
        """
        structure = Structure.Structure('predicted')
        model = Model.Model(0)
        chain = Chain.Chain('A')
        
        for i, (coord, aa) in enumerate(zip(coordinates, sequence)):
            residue = Residue.Residue((' ', i, ' '), aa, '')
            atom = Atom.Atom('CA', coord, 20.0, 1.0, ' ', 'CA', i, 'C')
            residue.add(atom)
            chain.add(residue)
        
        model.add(chain)
        structure.add(model)
        
        io = PDBIO()
        io.set_structure(structure)
        io.save(filename)



def main():
    # Example usage
    predictor = ProteinStructurePredictor()
    
    # Example sequence (shortened for demonstration)
    sequence = "MKWVTFISLLLLFSSAYSRGVFRRDAHKSEVAHRFKDLGEENFKALVLIAFAQYLQQCP"
    
    # Predict structure
    result = predictor.predict_structure(sequence)
    
    # Save structure
    predictor.save_structure(
        result['coordinates'],
        sequence,
        'predicted_structure.pdb'
    )
    
    print(f"Secondary structure prediction: {result['secondary_structure']}")
    print(f"Template used: {result['template_used']}")
    print("Structure has been saved to 'predicted_structure.pdb'")

if __name__ == "__main__":
    main()





######### Perform molecular docking simulations to predict the binding affinity between a protein target and a small molecule ligand ##########


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

df = pd.read_csv('polymerase_cluster-BI.csv')
df.head()

df.describe()

df.info()

df.isna().sum()

df.columns

X = df[['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',
       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',
       '23', '24', '25', '26', '27', '28', '29']]
y = df[['G1', 'G2', 'G3', 'G4', 'G5',
       'G6', 'G7', 'G8', 'G9', 'G10']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the model : {accuracy * 100}%")

# Accuracy Score
plt.figure(figsize=(4, 4))
plt.bar(['Accuracy'], [accuracy * 100], color='skyblue')
plt.ylim(0, 100)
plt.title('Model Accuracy')
plt.ylabel('Accuracy (%)')
plt.show()

class_report = classification_report(y_test, y_pred)
print("Classification Report :")
print(class_report)

# Confusion Matrix
from sklearn.metrics import ConfusionMatrixDisplay

# Create confusion matrix
cm = confusion_matrix(y_test.values.argmax(axis=1), y_pred.argmax(axis=1))

# Visualize confusion matrix
plt.figure(figsize=(10, 8))
ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()


############################### Write a program for pre-processing of a text document such as stop word removal, stemming. ####################

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

nltk.download('stopwords')
nltk.download('punkt')

text_document = "Text processing is an essential step in natural language processing. It includes tasks like stop word removal and stemming."

words = word_tokenize(text_document)
words

stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words if word.lower() not in stop_words]
filtered_words

stemmer = PorterStemmer()

stemmed_words = [stemmer.stem(word) for word in filtered_words]
stemmed_words

lemmatizer = WordNetLemmatizer()


lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]
lemmatized_words



######################################## mplement a program for retrieval of documents using inverted files. ############################

import re
from collections import defaultdict

documents = {
    1: "This is the first document. It contains some text.",
    2: "The second document is longer. It also contains some text.",
    3: "This is the third document.It is different from the first two.",
}

def preprocess_document(doc):
    #Convert to lowercaseand tokenize
    tokens = re.findall(r'\w+', doc.lower())
    #remove stop words
    stop_words = set(["is", "the", "it", "and", "some"])
    tokens = [token for token in tokens if token not in stop_words]
    return tokens


inverted_index = defaultdict(list)

for doc_id, doc_text in documents.items():
    tokens = preprocess_document(doc_text)
    for token in tokens:
        inverted_index[token].append(doc_id)

def retrieve_documents(query):
    query_tokens = preprocess_document(query)
    result = set()
    for token in query_tokens:
        if token in inverted_index:
            result.update(inverted_index[token])
    return list(result)

query = "document cntains text"
matching_documents = retrieve_documents(query)

if matching_documents:
    print(f"MAtching documents for query '{query}':")
    for doc_id in matching_documents:
        print(f"Document {doc_id}: {documents[doc_id]}")
else:
    print("No matching documents found.")



############################# Write a program to construct a Bayesian network considering medical data. #################################

import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import ParameterEstimator, BayesianEstimator
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination

data = pd.read_csv('heartdisease-IR.csv')
data.head()

data.tail()

data.info()

data.isnull().sum()

data.isna().sum()

data.describe()

data.columns

data.dtypes

model = BayesianNetwork([('age', 'heartdisease'),
                        ('Gender', 'heartdisease'),
                        ('Family', 'heartdisease'),
                        ('diet', 'heartdisease'),
                        ('Lifestyle', 'heartdisease'),
                        ('cholestrol', 'heartdisease')])

model.fit(data, estimator=MaximumLikelihoodEstimator)

HeartDiseasetest_infer = VariableElimination(model)

q1 = HeartDiseasetest_infer.query(variables=['heartdisease'], evidence = {'diet':1})
print(q1)

q2 = HeartDiseasetest_infer.query(variables=['heartdisease'], evidence={'age' : 4})
print(q2)

q4 = HeartDiseasetest_infer.query(variables=['heartdisease'], evidence={'cholestrol': 0})
print(q4)


################## Implement e-mail spam filtering using text classification algorithm with appropriate dataset #########################

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

df = pd.read_csv("email_spam (1)-IR.csv")

df.head()

df.isnull().sum()

df.drop_duplicates(inplace=True)
print("Duplicates removed.")

le = LabelEncoder()
df['spam'] = le.fit_transform(df['type'])

df['text'] = df['text'].replace("\n", "", regex=True)

df.head()

x_train, x_test, y_train, y_test = train_test_split(df['text'], df['spam'], test_size=0.1, random_state=42)


vectorizer = CountVectorizer()
x_train_count = vectorizer.fit_transform(x_train.values)
x_test_count = vectorizer.transform(x_test.values)


knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(x_train_count, y_train)

accuracy = knn_model.score(x_test_count, y_test)
print(f"Model accuracy: {accuracy }")

email = ['''Model Casting Call
Thank you for taking the time to register for the Anambra Fashion Expo 2023 Model Call. We are thrilled to have received your information and are excited to review your submission.

Our team will be carefully reviewing all of the applications we receive over the
next few weeks

Have you followed us on our social media handles?

Remember, one of the prerequisites for qualification is to follow all our social
media accounts and share all our content using the hashtag #AFE2023

You can follow us on Facebook, Instagram, and Twitter.

Facebook: https://facebook.com/AnambraFashionExpo2023
Instagram: https://instagram.com/anambrafashionexpo
Twitter: https://twitter.com/anambrafashion

In the meantime, we encourage you to stay connected and keep an eye out for
updates about the event. We will be posting!

Note: Create your personalized profile picture (DP) for the Model Casting of the Anambra Fashion Expo 2023

You can create your DP using the following link: https://getdp.co/afe2023

Best Regards,

Anambra Fashion Expo 2023 Team ''']
new_email_count = vectorizer.transform(email)
prediction = knn_model.predict(new_email_count)

if prediction[0] == 1:
    print("The email is Spam.")
else:
    print("The email is Not Spam.")





#####################  Implement Agglomerative hierarchical clustering algorithm using appropriate dataset. ########################

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from scipy.cluster.hierarchy import dendrogram, linkage

iris = load_iris()
X = iris.data
y = iris.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

n_clusters = 3
agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
cluster_labels = agg_clustering.fit_predict(X_pca)

linked = linkage(X_pca, 'ward')
plt.figure(figsize=(12,6))
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title("Dendrogram")
plt.xlabel("Cluster Size")
plt.ylabel('Distance')
plt.show()

print("Cluster Labels")
print(cluster_labels)

plt.scatter(X_pca[:,0],X_pca[:, 1], c=cluster_labels, cmap='rainbow')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Agglomerative Hierarchical Clustering')
plt.show()

